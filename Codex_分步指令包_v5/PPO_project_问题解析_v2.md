# PPO_project 训练效果退化问题解析（对比 PPO最终版_改进.py）

> 用途说明：这份文档主要给你定位“为什么重构后退化”。**它不是逐阶段给 Codex 的执行指令**。
> 给 Codex 时以 `00_README + P0..P4` 为准；本解析最多在第一次作为背景贴一次即可，避免它跳关/越权改动。


## 1. 现象复述（你给的截图）
- 回合奖励长期停在较大的负值区间，基本没有“越训越好”的趋势。
- 轨迹能起步但会逐渐偏离参考直线（红色轨迹向下弯），并且通常到不了终点（进度/终点奖励吃不到）。
- Critic loss 有上升/高位震荡迹象，说明价值函数学得不稳，优势估计噪声大，PPO 更新等于“在雾里开车”。

结论先说：这不像是“PPO 不行”，更像是**重构后 MDP（动作语义/尺度/归一化/约束）和旧版不一致**，导致奖励与动作的对应关系被扭曲，学习信号变弱甚至错向。

---

## 2. 与原始脚本相比，重构版最致命的差异（按优先级排序）

### P0-1 动作语义发生了“系统级换壳”
原始脚本的关键设计是：
- **每一步的有效运动方向 = 当前路径切向 + 角速度动作×dt**（动作是“相对路径切向”的微调）  
  这能把问题变成“沿路径走 + 小修正”，探索空间小、学习更稳定。

重构版里（PPO_project 的 Env）把角度更新改成了“累计航向积分”（相当于把动作变成绝对航向控制）。  
这会导致：同样的策略输出，在新环境里会出现**误差积累、越走越偏、拐角处更容易跑飞**。

**影响：** 这是最容易造成“怎么训都好不了”的差异，因为你实际上把任务从“跟踪”改成了“导航+跟踪”。

### P0-2 动作裁剪/约束导致“行为-梯度不一致”
重构版通常会做更激进的 clip（或把动作映射到更窄范围）。只要出现：
- 环境执行的是 clip/约束后的动作；
- PPO 更新却使用 clip 前动作的 log_prob；

就会产生 **action mismatch**：梯度在优化一个“没被执行”的动作，学习信号被系统性污染。  
原始脚本也有 KCM（动力学约束），但如果新版本裁剪更硬、更频繁，这个问题会被放大。

### P0-3 关键尺度/归一化/配置漂移（尤其是 dt、MAX_*）
如果重构版把 dt、MAX_VEL 等参数“写死/覆盖配置”，会出现两类隐蔽问题：
- 速度/角速度在状态里被归一化到极小量，网络几乎看不见它；
- 奖励里用 MAX_VEL 做比例（velocity_ratio），但动作范围又达不到想要的比例，导致速度相关奖励恒定偏差。

这类漂移不会直接报错，却会把学习“悄悄掰弯”。

### P1 双重归一化（Env 已归一化 + 训练又做 StateNormalizer）
Env 本身已经输出归一化状态，再额外做 RunningMeanStd 标准化，会引入额外的非平稳性。  
它不一定会让训练完全失败，但在上述 P0 问题存在时，会雪上加霜。

---

## 3. “为什么会表现为探索到最后也没变好？”（机制解释）
把训练想象成：策略输出动作 → 环境执行 → 得到奖励 → PPO 用 log_prob 反推该增大/减小哪些动作概率。

当出现：
1) 动作语义换了（相对切向 → 绝对航向积分），策略的“直觉”全部失效；
2) clip/约束让执行动作 ≠ 用来算 log_prob 的动作，梯度指向错误；
3) 状态/奖励尺度漂移让关键信号变弱；

结果就是：PPO 的更新要么非常小（学不到），要么朝错方向（越学越歪），于是曲线看起来像“收敛到一坨坏局部最优”。

---

## 4. 先恢复到原始代码效果：最短修复路径（强建议按这个顺序做）

### Step A：做一个“行为一致性回归测试”
写一个脚本：同一个 Pm、同一个 seed、同一串动作序列，分别跑：
- 原始 Env（从 PPO最终版_改进.py 抽出 Env 类放到 legacy_env.py）
- 重构 Env（src/environment/cnc_env.py）
对比每一步：position、tau、progress、reward 的差异。  
目标：误差 < 1e-6（或至少在数值容忍范围内）。

> 没有这个回归测试，你后面修会像打地鼠。

### Step B：把动作语义改回“相对路径切向”
把重构 Env 的 apply_action / position update 改回：
- path_angle = _get_path_direction(current_position)
- effective_angle = path_angle + theta_prime * dt
- 位移 = length_prime * dt

### Step C：修复 action mismatch（policy_action / exec_action / log_prob 对齐）
核心原则：PPO 的 log_prob 必须对应“策略采样并送入环境的动作”（action_policy）。  
环境内部经 KCM/安全过滤得到的 action_exec 只能用于执行与诊断，不允许替换成 PPO 更新动作。

强制落地：
- 在 env.step 中记录两套动作：`action_policy`（送入 env 的）与 `action_exec`（实际执行的）。
- rollout/buffer 只存 `action_policy`，并用它计算/存 log_prob。
- 打印 `|action_exec - action_policy|` 的偏差统计，防止 KCM 过强把学习信号掐死。

实现路线二选一：
1) **最推荐**：env 内不做额外硬 clip（除了 action_space 边界裁剪），KCM 作为唯一安全过滤产生 `action_exec`；PPO 始终用 `action_policy`。  
2) 若必须用 tanh-squash 等有界分布：在 policy 中实现 squashed Gaussian，用“有界后的 action_policy”计算 log_prob；env 仍可产生 action_exec，但禁止用 action_exec 反算 log_prob。

### Step D：取消“写死参数”，让配置完全生效
dt、MAX_*、epsilon 等只来自 config，不做 legacy 覆盖。

### Step E：默认关闭 StateNormalizer（或只在 warmup 后冻结）
先复现原始效果，再考虑是否需要额外标准化。

---

## 5. 在原始效果基础上实现你的新需求：路线图

### 需求 1：直线快速进给
核心：速度奖励不要固定追 0.7，而是**随“前方曲率/拐角角度/到拐点距离”动态调整目标速度**。
- 直线：speed_target → 0.9~1.0
- 入弯前：speed_target 逐步下降
- 出弯后：speed_target 快速回升（exit boost）

### 需求 2：拐角最优内切路径（误差-效率折中）
给 PPO 一个“可学的几何捷径”，而不是死跟中心线：
- 识别拐角方向（左/右）与转角大小；
- 在距离拐点 < d_turn 时，把“参考路径”从中心线逐步偏向内侧边界；
- 定义一个可调的“内切偏置量” d_offset ∈ [0, half_epsilon]，并把它作为：
  - 状态特征（让策略知道自己在切多少）
  - 奖励项（切得好：路程更短/速度更高，但不能越界）

实现上最稳的做法是：**构造一个局部“内切虚拟参考线/参考弧”**，让 contour_error 对这个虚拟参考计算；并且用权重在“中心线误差”和“内切误差”之间平滑切换。

### 需求 3：快速出弯 & 必须到终点
- 出弯检测：segment_idx 发生拐角跃迁后的 N 步，给“加速+减小误差”额外奖励；
- 到终点：
  - completion reward 再强化（progress>0.8 时的 proximity bonus 更陡）
  - 增加“停滞惩罚”：连续 K 步 progress_diff≈0 → 逐步扣分或提前终止

---

## 6. 验收标准（建议你用这些指标卡住 Codex 输出）
1) **复现**：同配置同随机种子下，重构版训练曲线与原始版同量级；短训练（例如 200~500 episode）即可出现明显提升趋势。
2) **到终点**：>=95% episode 在 max_steps 内满足 done（到达终点判据）。
3) **直线速度**：直线段平均速度比（v/MAX_VEL）显著高于拐角段，且拐角前后速度曲线符合“入弯降速-出弯加速”。
4) **内切行为**：在转角处，轨迹相对中心线存在稳定的向内偏置，但不越过边界；RMSE 不显著恶化。
