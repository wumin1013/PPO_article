# 指令文档 P0：先对齐 PPO 最终版（让训练跑起来 + 解决“停着拿分”）

**适用范围**：`PPO_project` 代码库（模块化版本）。  
**目标**：恢复可学习性——智能体必须推进才能高分，并且行为语义与单文件最终版一致，作为后续“平滑拐角”的底座。  
**核心原则**：只保留物理约束（KCM）；把人写死的规则（speed cap / braking envelope）先软化或关闭。

---

## Step 0（必做）建立回归基线
1. 固定 config：使用你当前的 `saved_models/exp_train_line_20251224_225648/.../config.yaml` 复制一份为 `configs/train_square_p0.yaml`
2. 记录 3 个基线指标（当前版本）：
   - 100 回合成功率（reached_target / lap_completed）
   - 平均 episode 步数
   - 平均 progress_final

**验收**：只要能复现“当前问题”（成功率低、progress 不增长）即可。

---

## Step 1 改造动作语义：从“纯航向积分”改回“参考切向 + 小修正”
### 1.1 修改文件
- 文件：`PPO_project/src/environment/cnc_env.py`
- 函数：`calculate_new_position()`

### 1.2 改造要点
把当前逻辑：
- `heading += omega_exec * dt`
替换为：
- 计算参考方向 `theta_ref`（先用“当前路径切向”，后续 P1 再升级为 lookahead）
- `effective_angle = theta_ref + omega_exec * dt`
- 用 `effective_angle` 做位移积分
- **不要把 heading 状态长期积分**（或保留但仅用于调试，不作为状态演化主导）

### 1.3 伪代码（给 Codex）
```python
def calculate_new_position(self, omega_exec, v_exec):
    dt = self.interpolation_period
    theta_ref = self._get_path_direction(self.current_position)  # P0：先复现最终版语义
    effective = theta_ref + omega_exec * dt
    self._current_direction_angle = effective  # 仅记录
    disp = v_exec * dt
    return self.current_position + disp * np.array([np.cos(effective), np.sin(effective)], float)
```

---

## Step 2 改造奖励：推进必须成为主要收益（消灭“停着拿分”）
### 2.1 修改文件
- 文件：`PPO_project/src/environment/reward.py`
- 函数：`RewardCalculator.calculate_reward()`

### 2.2 改造原则
1. **把 tracking/direction 的“正向基线”移除**：改成纯惩罚（误差越大越罚），误差为 0 时不额外给分。
2. **progress 用“每步增量”直接给正奖励**，并显著大于其它项。
3. **time penalty 绝对值增大**，确保不推进时净收益为负。
4. 速度奖励只在“确实推进”时给（避免 0 速度也得分）。

### 2.3 推荐形式（稳定且可控）
- `r_progress = + w_s * progress_diff`
- `r_track = - w_e * (e/half_epsilon)^2`
- `r_dir = - w_tau * tau^2`
- `r_time = - w_t`（常数）
- `r_speed = + w_v * v_ratio * progress_diff`（可选）

> 注：`progress_diff` 已在函数里计算得到（你现有代码用 `progress - self.last_progress`）。

### 2.4 配置建议（初始）
- `w_s = 80 ~ 150`
- `w_e = 5 ~ 20`（直线段后续再加大）
- `w_tau = 1 ~ 5`
- `w_t = 0.5 ~ 2.0`（比现在的 0.01 大两个数量级）
- `w_v = 0 ~ 50`（先可为 0）

---

## Step 3（临时）关闭强干预规则：让 RL 自己学
在 `configs/train_square_p0.yaml` 里把以下开关关掉：

- `reward_weights.corridor.enabled: false`（先关）
- `reward_weights.p4.speed_cap_enabled: false`
- `reward_weights.p6_1.v_target_smoother_enabled: false`
- `reward_weights.p7_3.kappa_smoothing_enabled: false`
- `reward_weights.p8.use_recovery_cap: false`
- `reward_weights.p8.use_vcap_rate_limit: false`

保留：
- `experiment.enable_kcm: true`
- 终止条件：OOB（误差 > half_epsilon）保留

---

## Step 4 验收标准（P0 结束条件）
在相同训练预算（例如 300~500 episodes）下，满足：

1. **stall 触发率 < 5%**
2. **平均 progress_final > 0.95**（闭合方形可用 lap_completed 统计）
3. 轨迹能完成一圈/到终点，且大多数时间误差在带内

如果 P0 未通过：
- 优先加大 `w_t`（时间惩罚）和 `w_s`（推进奖励）
- 其次加大 OOB 边界到 `0.8*half_epsilon`（让探索更难出界）——但这是“训练友好”措施，后续再收紧。

---

# P0 的产出
通过 P0 后，你会得到一个“会跑、会推进、会贴线”的 PPO 基座。  
下一步 P1 才开始让拐角变圆滑。
