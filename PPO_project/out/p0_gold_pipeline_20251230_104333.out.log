[SEED] global seed set to 42 (random/numpy/torch)
加载配置: C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed42.yaml
environment:
  epsilon: 1.5
  interpolation_period: 0.001
  lookahead_points: 8
  max_steps: 4000
experiment:
  baseline_type: null
  category: p0_seed42
  enable_kcm: true
  mode: train
  model_path: null
  name: p0_seed42
  seed: 42
kinematic_constraints:
  MAX_ACC: 2000.0
  MAX_ANG_ACC: 100.0
  MAX_ANG_JERK: 1000.0
  MAX_ANG_VEL: 6.283185307179586
  MAX_JERK: 20000.0
  MAX_VEL: 100.0
path:
  closed: true
  num_points: 200
  scale: 10.0
  square: {}
  type: square
ppo:
  actor_lr: 2.0e-05
  batch_size: null
  critic_lr: 0.0001
  ent_coef: 0.01
  epochs: 10
  eps: 0.1
  gamma: 0.99
  hidden_dim: 256
  lmbda: 0.95
reward_weights:
  corridor:
    barrier_scale_ratio: 0.05
    barrier_weight: 2.0
    center_power: 1.0
    center_weight: 1.2
    dir_pref_beta: 2.0
    dir_pref_weight: 1.0
    dist_enter: 6.0
    dist_exit: 8.0
    enabled: false
    exit_center_ramp_steps: 60
    heading_weight: 2.0
    margin_ratio: 0.1
    outside_penalty_weight: 20.0
    safe_margin_ratio: 0.2
    theta_enter_deg: 12.0
    theta_exit_deg: 6.0
  p4:
    d_scale: null
    debug: false
    exit_boost_enabled: true
    exit_progress_mult: 1.35
    exit_speed_target_min: 0.95
    exit_window_sec: 0.25
    p7_2_k: 0.01
    speed_cap_enabled: false
    speed_cap_eps: 1e-12
    speed_cap_k: 4
    speed_cap_preview_points: 8
    speed_cap_s_max: 30.0
    speed_cap_s_min: 1.0
    speed_cap_use_wddot: true
    speed_cap_use_wdot: true
    speed_weight: 6.0
    stall_enabled: true
    stall_penalty: -8.0
    stall_progress_eps: 1e-4
    stall_steps: 300
    stall_v_eps: 0.05
    theta_max_deg: 90.0
    time_penalty: -0.01
    v_max: 1.0
    v_min: 0.35
  p6_1:
    a_ref_max_ratio: 0.5
    du_enabled: true
    du_mode: l1
    j_ref_enabled: true
    j_ref_max_ratio: 0.5
    v_target_mode: accel
    v_target_smoother_enabled: false
    v_target_tau: 0.1
    w_du: 0.01
  p7_3:
    kappa_dkappa_limit: null
    kappa_smoothing_beta: 0.25
    kappa_smoothing_enabled: false
    stall_cap_low: 0.25
    trace_ring_size: 200
  p8:
    ang_cap_min_ratio: 0.1
    corner_exit_e_release_ratio: 0.5
    corner_exit_hold_steps: 3
    corner_exit_psi_release_deg: 30.0
    corner_mode_ignore_geom_cap: false
    corner_off_dist_scale: 4.0
    corner_on_dist_scale: 2.0
    recovery_e_release_ratio: 0.5
    recovery_e_warn_ratio: 0.75
    recovery_vcap: 0.1
    straight_mode_ignore_geom_cap: false
    use_corner_exit_hysteresis: true
    use_recovery_cap: false
    use_vcap_rate_limit: true
    vcap_rate_down: 0.05
    vcap_rate_up: 0.1
  w_e: 5.0
  w_s: 20.0
  w_smooth: 0.05
  w_t: 1.0
  w_tau: 2.0
seed: 42
training:
  checkpoint_interval_steps: 2048
  log_interval: 50
  num_episodes: 1000
  save_interval: 100
  smoothing_factor: 0.9
  use_obs_normalizer: false

使用设备: cuda
使用参数化路径 square, 采样点数: 200
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[RUN] seed=42 mode=train dt=0.001 gamma=0.990000 H_steps≈100.0 H_time≈0.1000
[RUN] kinematic_constraints: MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
环境创建成功: 状态维度36, 动作维度=2
[OBS] Env 返回 normalized obs（训练端禁用 StateNormalizer）
PPO智能体创建成功 模式: train
断点续训: 从 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed42\P0_gold_20251230_034122\checkpoints\latest_checkpoint.pth 恢复，已完成 episode=999, global_step=918995, best_eval_reward=-932.78
续训起始回合(1000)已达到总回合数(1000)，不再继续。
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[p0_eval] 1/50 done_reason=success progress_final=1.0000 steps=731 elapsed=7.8s
[p0_eval] 5/50 done_reason=success progress_final=1.0000 steps=731 elapsed=41.8s
[p0_eval] 10/50 done_reason=success progress_final=1.0000 steps=731 elapsed=127.1s
[p0_eval] 15/50 done_reason=success progress_final=1.0000 steps=731 elapsed=166.5s
[p0_eval] 20/50 done_reason=success progress_final=1.0000 steps=731 elapsed=207.4s
[p0_eval] 25/50 done_reason=success progress_final=1.0000 steps=731 elapsed=248.4s
[p0_eval] 30/50 done_reason=success progress_final=1.0000 steps=731 elapsed=288.6s
[p0_eval] 35/50 done_reason=success progress_final=1.0000 steps=731 elapsed=327.9s
[p0_eval] 40/50 done_reason=success progress_final=1.0000 steps=731 elapsed=369.4s
[p0_eval] 45/50 done_reason=success progress_final=1.0000 steps=731 elapsed=409.0s
[p0_eval] 50/50 done_reason=success progress_final=1.0000 steps=731 elapsed=449.8s
{
  "phase": "p0_eval",
  "passed": true,
  "episodes": 50,
  "model_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed42\\P0_gold_20251230_034122\\checkpoints\\best_model.pth",
  "deterministic": true,
  "success_rate": 1.0,
  "stall_rate": 0.0,
  "mean_progress_final": 1.0,
  "max_abs_contour_error": 0.06379457379489217,
  "has_non_finite": false,
  "thresholds": {
    "success_rate_ge": 0.8,
    "stall_rate_le": 0.05,
    "mean_progress_final_ge": 0.95,
    "max_abs_contour_error_le": 0.75
  },
  "half_epsilon": 0.75,
  "timestamp": "2025-12-30 10:51:12",
  "config_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\configs\\train_square_p0_seed42.yaml"
}
[SEED] global seed set to 43 (random/numpy/torch)
加载配置: C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed43.yaml
environment:
  epsilon: 1.5
  interpolation_period: 0.001
  lookahead_points: 8
  max_steps: 4000
experiment:
  baseline_type: null
  category: p0_seed43
  enable_kcm: true
  mode: train
  model_path: null
  name: p0_seed43
  seed: 43
kinematic_constraints:
  MAX_ACC: 2000.0
  MAX_ANG_ACC: 100.0
  MAX_ANG_JERK: 1000.0
  MAX_ANG_VEL: 6.283185307179586
  MAX_JERK: 20000.0
  MAX_VEL: 100.0
path:
  closed: true
  num_points: 200
  scale: 10.0
  square: {}
  type: square
ppo:
  actor_lr: 2.0e-05
  batch_size: null
  critic_lr: 0.0001
  ent_coef: 0.01
  epochs: 10
  eps: 0.1
  gamma: 0.99
  hidden_dim: 256
  lmbda: 0.95
reward_weights:
  corridor:
    barrier_scale_ratio: 0.05
    barrier_weight: 2.0
    center_power: 1.0
    center_weight: 1.2
    dir_pref_beta: 2.0
    dir_pref_weight: 1.0
    dist_enter: 6.0
    dist_exit: 8.0
    enabled: false
    exit_center_ramp_steps: 60
    heading_weight: 2.0
    margin_ratio: 0.1
    outside_penalty_weight: 20.0
    safe_margin_ratio: 0.2
    theta_enter_deg: 12.0
    theta_exit_deg: 6.0
  p4:
    d_scale: null
    debug: false
    exit_boost_enabled: true
    exit_progress_mult: 1.35
    exit_speed_target_min: 0.95
    exit_window_sec: 0.25
    p7_2_k: 0.01
    speed_cap_enabled: false
    speed_cap_eps: 1e-12
    speed_cap_k: 4
    speed_cap_preview_points: 8
    speed_cap_s_max: 30.0
    speed_cap_s_min: 1.0
    speed_cap_use_wddot: true
    speed_cap_use_wdot: true
    speed_weight: 6.0
    stall_enabled: true
    stall_penalty: -8.0
    stall_progress_eps: 1e-4
    stall_steps: 300
    stall_v_eps: 0.05
    theta_max_deg: 90.0
    time_penalty: -0.01
    v_max: 1.0
    v_min: 0.35
  p6_1:
    a_ref_max_ratio: 0.5
    du_enabled: true
    du_mode: l1
    j_ref_enabled: true
    j_ref_max_ratio: 0.5
    v_target_mode: accel
    v_target_smoother_enabled: false
    v_target_tau: 0.1
    w_du: 0.01
  p7_3:
    kappa_dkappa_limit: null
    kappa_smoothing_beta: 0.25
    kappa_smoothing_enabled: false
    stall_cap_low: 0.25
    trace_ring_size: 200
  p8:
    ang_cap_min_ratio: 0.1
    corner_exit_e_release_ratio: 0.5
    corner_exit_hold_steps: 3
    corner_exit_psi_release_deg: 30.0
    corner_mode_ignore_geom_cap: false
    corner_off_dist_scale: 4.0
    corner_on_dist_scale: 2.0
    recovery_e_release_ratio: 0.5
    recovery_e_warn_ratio: 0.75
    recovery_vcap: 0.1
    straight_mode_ignore_geom_cap: false
    use_corner_exit_hysteresis: true
    use_recovery_cap: false
    use_vcap_rate_limit: true
    vcap_rate_down: 0.05
    vcap_rate_up: 0.1
  w_e: 5.0
  w_s: 20.0
  w_smooth: 0.05
  w_t: 1.0
  w_tau: 2.0
seed: 43
training:
  checkpoint_interval_steps: 2048
  log_interval: 50
  num_episodes: 1000
  save_interval: 100
  smoothing_factor: 0.9
  use_obs_normalizer: false

使用设备: cuda
使用参数化路径 square, 采样点数: 200
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[RUN] seed=43 mode=train dt=0.001 gamma=0.990000 H_steps≈100.0 H_time≈0.1000
[RUN] kinematic_constraints: MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
环境创建成功: 状态维度36, 动作维度=2
[OBS] Env 返回 normalized obs（训练端禁用 StateNormalizer）
PPO智能体创建成功 模式: train

开始训练 共1000个回合

发现更优模型: eval_reward=-1063.75, 保存到 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'

================================================================================
Episode 50 - 论文指标摘要:
================================================================================
  RMSE Error:              0.052695
  Mean Jerk:               19940.830491
  Roughness Proxy:         346023.287773
  Mean Velocity:           13.4257
  Max Error:               0.116983
  Mean KCM Intervention:   0.856913
  Steps:                   2924
  Progress:                1.0000
  Total Reward:            -3282.51
================================================================================

发现更优模型: eval_reward=-675.06, 保存到 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth
发现更优模型: eval_reward=-659.45, 保存到 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
发现更优模型: eval_reward=-656.07, 保存到 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'

================================================================================
Episode 100 - 论文指标摘要:
================================================================================
  RMSE Error:              0.033675
  Mean Jerk:               19783.263095
  Roughness Proxy:         400000.000000
  Mean Velocity:           7.1505
  Max Error:               0.043474
  Mean KCM Intervention:   0.920993
  Steps:                   1661
  Progress:                0.2990
  Total Reward:            -1841.73
================================================================================

发现更优模型: eval_reward=-576.91, 保存到 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'

================================================================================
Episode 150 - 论文指标摘要:
================================================================================
  RMSE Error:              0.057718
  Mean Jerk:               19966.901117
  Roughness Proxy:         80000.000000
  Mean Velocity:           16.2169
  Max Error:               0.124674
  Mean KCM Intervention:   0.873446
  Steps:                   2417
  Progress:                1.0000
  Total Reward:            -2724.58
================================================================================

写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'
写入 latest_trajectory.csv 失败，已跳过本回合: [WinError 5] 拒绝访问。: 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv.tmp' -> 'C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\logs\\latest_trajectory.csv'

================================================================================