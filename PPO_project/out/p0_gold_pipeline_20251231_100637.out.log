[SEED] global seed set to 42 (random/numpy/torch)
加载配置: C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed42.yaml
environment:
  epsilon: 1.5
  interpolation_period: 0.001
  lookahead_points: 8
  max_steps: 4000
experiment:
  baseline_type: null
  category: p0_seed42
  enable_kcm: true
  mode: train
  model_path: null
  name: p0_seed42
  seed: 42
kinematic_constraints:
  MAX_ACC: 2000.0
  MAX_ANG_ACC: 100.0
  MAX_ANG_JERK: 1000.0
  MAX_ANG_VEL: 6.283185307179586
  MAX_JERK: 20000.0
  MAX_VEL: 100.0
path:
  closed: true
  num_points: 200
  scale: 10.0
  square: {}
  type: square
ppo:
  actor_lr: 2.0e-05
  batch_size: null
  critic_lr: 0.0001
  ent_coef: 0.01
  epochs: 10
  eps: 0.1
  gamma: 0.99
  hidden_dim: 256
  lmbda: 0.95
reward_weights:
  corridor:
    barrier_scale_ratio: 0.05
    barrier_weight: 2.0
    center_power: 1.0
    center_weight: 1.2
    dir_pref_beta: 2.0
    dir_pref_weight: 1.0
    dist_enter: 6.0
    dist_exit: 8.0
    enabled: false
    exit_center_ramp_steps: 60
    heading_weight: 2.0
    margin_ratio: 0.1
    outside_penalty_weight: 20.0
    safe_margin_ratio: 0.2
    theta_enter_deg: 12.0
    theta_exit_deg: 6.0
  p4:
    d_scale: null
    debug: false
    exit_boost_enabled: true
    exit_progress_mult: 1.35
    exit_speed_target_min: 0.95
    exit_window_sec: 0.25
    p7_2_k: 0.01
    speed_cap_enabled: false
    speed_cap_eps: 1e-12
    speed_cap_k: 4
    speed_cap_preview_points: 8
    speed_cap_s_max: 30.0
    speed_cap_s_min: 1.0
    speed_cap_use_wddot: true
    speed_cap_use_wdot: true
    speed_weight: 6.0
    stall_enabled: true
    stall_penalty: -8.0
    stall_progress_eps: 1e-4
    stall_steps: 300
    stall_v_eps: 0.05
    theta_max_deg: 90.0
    time_penalty: -0.01
    v_max: 1.0
    v_min: 0.35
  p6_1:
    a_ref_max_ratio: 0.5
    du_enabled: true
    du_mode: l1
    j_ref_enabled: true
    j_ref_max_ratio: 0.5
    v_target_mode: accel
    v_target_smoother_enabled: false
    v_target_tau: 0.1
    w_du: 0.01
  p7_3:
    kappa_dkappa_limit: null
    kappa_smoothing_beta: 0.25
    kappa_smoothing_enabled: false
    stall_cap_low: 0.25
    trace_ring_size: 200
  p8:
    ang_cap_min_ratio: 0.1
    corner_exit_e_release_ratio: 0.5
    corner_exit_hold_steps: 3
    corner_exit_psi_release_deg: 30.0
    corner_mode_ignore_geom_cap: false
    corner_off_dist_scale: 4.0
    corner_on_dist_scale: 2.0
    recovery_e_release_ratio: 0.5
    recovery_e_warn_ratio: 0.75
    recovery_vcap: 0.1
    straight_mode_ignore_geom_cap: false
    use_corner_exit_hysteresis: true
    use_recovery_cap: false
    use_vcap_rate_limit: true
    vcap_rate_down: 0.05
    vcap_rate_up: 0.1
  w_e: 5.0
  w_s: 20.0
  w_smooth: 0.05
  w_t: 1.0
  w_tau: 2.0
seed: 42
training:
  checkpoint_interval_steps: 2048
  log_interval: 50
  num_episodes: 1000
  save_interval: 100
  smoothing_factor: 0.9
  use_obs_normalizer: false

使用设备: cuda
使用参数化路径 square, 采样点数: 200
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[RUN] seed=42 mode=train dt=0.001 gamma=0.990000 H_steps≈100.0 H_time≈0.1000
[RUN] kinematic_constraints: MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
环境创建成功: 状态维度36, 动作维度=2
[OBS] Env 返回 normalized obs（训练端禁用 StateNormalizer）
PPO智能体创建成功 模式: train
断点续训: 从 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed42\P0_gold_20251230_034122\checkpoints\latest_checkpoint.pth 恢复，已完成 episode=999, global_step=918995, best_eval_reward=-932.78
续训起始回合(1000)已达到总回合数(1000)，不再继续。
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[p0_eval] 1/50 done_reason=success progress_final=1.0000 steps=928 elapsed=10.3s
[p0_eval] 5/50 done_reason=success progress_final=1.0000 steps=922 elapsed=62.3s
[p0_eval] 10/50 done_reason=success progress_final=1.0000 steps=901 elapsed=172.6s
[p0_eval] 15/50 done_reason=success progress_final=1.0000 steps=929 elapsed=282.4s
[p0_eval] 20/50 done_reason=success progress_final=1.0000 steps=949 elapsed=392.0s
[p0_eval] 25/50 done_reason=success progress_final=1.0000 steps=902 elapsed=497.8s
[p0_eval] 30/50 done_reason=success progress_final=1.0000 steps=910 elapsed=605.9s
[p0_eval] 35/50 done_reason=success progress_final=1.0000 steps=880 elapsed=714.0s
[p0_eval] 40/50 done_reason=success progress_final=1.0000 steps=920 elapsed=822.2s
[p0_eval] 45/50 done_reason=success progress_final=1.0000 steps=959 elapsed=932.0s
[p0_eval] 50/50 done_reason=success progress_final=1.0000 steps=898 elapsed=1041.3s
{
  "phase": "p0_eval",
  "passed": true,
  "episodes": 50,
  "model_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed42\\P0_gold_20251230_034122\\checkpoints\\best_model.pth",
  "deterministic": false,
  "seed_eval": 42,
  "episode_set": null,
  "success_rate": 1.0,
  "stall_rate": 0.0,
  "mean_progress_final": 1.0,
  "max_abs_contour_error": 0.06469480202631317,
  "has_non_finite": false,
  "thresholds": {
    "success_rate_ge": 0.8,
    "stall_rate_le": 0.05,
    "mean_progress_final_ge": 0.95,
    "max_abs_contour_error_le": 0.75
  },
  "half_epsilon": 0.75,
  "timestamp": "2025-12-31 10:24:08",
  "config_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\configs\\train_square_p0_seed42.yaml"
}
[SEED] global seed set to 43 (random/numpy/torch)
加载配置: C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed43.yaml
environment:
  epsilon: 1.5
  interpolation_period: 0.001
  lookahead_points: 8
  max_steps: 4000
experiment:
  baseline_type: null
  category: p0_seed43
  enable_kcm: true
  mode: train
  model_path: null
  name: p0_seed43
  seed: 43
kinematic_constraints:
  MAX_ACC: 2000.0
  MAX_ANG_ACC: 100.0
  MAX_ANG_JERK: 1000.0
  MAX_ANG_VEL: 6.283185307179586
  MAX_JERK: 20000.0
  MAX_VEL: 100.0
path:
  closed: true
  num_points: 200
  scale: 10.0
  square: {}
  type: square
ppo:
  actor_lr: 2.0e-05
  batch_size: null
  critic_lr: 0.0001
  ent_coef: 0.01
  epochs: 10
  eps: 0.1
  gamma: 0.99
  hidden_dim: 256
  lmbda: 0.95
reward_weights:
  corridor:
    barrier_scale_ratio: 0.05
    barrier_weight: 2.0
    center_power: 1.0
    center_weight: 1.2
    dir_pref_beta: 2.0
    dir_pref_weight: 1.0
    dist_enter: 6.0
    dist_exit: 8.0
    enabled: false
    exit_center_ramp_steps: 60
    heading_weight: 2.0
    margin_ratio: 0.1
    outside_penalty_weight: 20.0
    safe_margin_ratio: 0.2
    theta_enter_deg: 12.0
    theta_exit_deg: 6.0
  p4:
    d_scale: null
    debug: false
    exit_boost_enabled: true
    exit_progress_mult: 1.35
    exit_speed_target_min: 0.95
    exit_window_sec: 0.25
    p7_2_k: 0.01
    speed_cap_enabled: false
    speed_cap_eps: 1e-12
    speed_cap_k: 4
    speed_cap_preview_points: 8
    speed_cap_s_max: 30.0
    speed_cap_s_min: 1.0
    speed_cap_use_wddot: true
    speed_cap_use_wdot: true
    speed_weight: 6.0
    stall_enabled: true
    stall_penalty: -8.0
    stall_progress_eps: 1e-4
    stall_steps: 300
    stall_v_eps: 0.05
    theta_max_deg: 90.0
    time_penalty: -0.01
    v_max: 1.0
    v_min: 0.35
  p6_1:
    a_ref_max_ratio: 0.5
    du_enabled: true
    du_mode: l1
    j_ref_enabled: true
    j_ref_max_ratio: 0.5
    v_target_mode: accel
    v_target_smoother_enabled: false
    v_target_tau: 0.1
    w_du: 0.01
  p7_3:
    kappa_dkappa_limit: null
    kappa_smoothing_beta: 0.25
    kappa_smoothing_enabled: false
    stall_cap_low: 0.25
    trace_ring_size: 200
  p8:
    ang_cap_min_ratio: 0.1
    corner_exit_e_release_ratio: 0.5
    corner_exit_hold_steps: 3
    corner_exit_psi_release_deg: 30.0
    corner_mode_ignore_geom_cap: false
    corner_off_dist_scale: 4.0
    corner_on_dist_scale: 2.0
    recovery_e_release_ratio: 0.5
    recovery_e_warn_ratio: 0.75
    recovery_vcap: 0.1
    straight_mode_ignore_geom_cap: false
    use_corner_exit_hysteresis: true
    use_recovery_cap: false
    use_vcap_rate_limit: true
    vcap_rate_down: 0.05
    vcap_rate_up: 0.1
  w_e: 5.0
  w_s: 20.0
  w_smooth: 0.05
  w_t: 1.0
  w_tau: 2.0
seed: 43
training:
  checkpoint_interval_steps: 2048
  log_interval: 50
  num_episodes: 1000
  save_interval: 100
  smoothing_factor: 0.9
  use_obs_normalizer: false

使用设备: cuda
使用参数化路径 square, 采样点数: 200
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[RUN] seed=43 mode=train dt=0.001 gamma=0.990000 H_steps≈100.0 H_time≈0.1000
[RUN] kinematic_constraints: MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
环境创建成功: 状态维度36, 动作维度=2
[OBS] Env 返回 normalized obs（训练端禁用 StateNormalizer）
PPO智能体创建成功 模式: train
断点续训: 从 C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\latest_checkpoint.pth 恢复，已完成 episode=999, global_step=2641074, best_eval_reward=-389.68
续训起始回合(1000)已达到总回合数(1000)，不再继续。
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[p0_eval] 1/50 done_reason=stall progress_final=0.5122 steps=2108 elapsed=42.9s
[p0_eval] 5/50 done_reason=success progress_final=1.0000 steps=2853 elapsed=273.7s
[p0_eval] 10/50 done_reason=success progress_final=1.0000 steps=2736 elapsed=606.6s
[p0_eval] 15/50 done_reason=success progress_final=1.0000 steps=2809 elapsed=862.5s
[p0_eval] 20/50 done_reason=success progress_final=1.0000 steps=3582 elapsed=1170.2s
[p0_eval] 25/50 done_reason=success progress_final=1.0000 steps=3600 elapsed=1589.5s
[p0_eval] 30/50 done_reason=stall progress_final=0.2404 steps=746 elapsed=1845.2s
[p0_eval] 35/50 done_reason=success progress_final=1.0000 steps=2928 elapsed=2165.9s
[p0_eval] 40/50 done_reason=success progress_final=1.0000 steps=3331 elapsed=2676.9s
[p0_eval] 45/50 done_reason=success progress_final=1.0000 steps=2918 elapsed=3111.8s
[p0_eval] 50/50 done_reason=success progress_final=1.0000 steps=3286 elapsed=3537.8s
{
  "phase": "p0_eval",
  "passed": false,
  "episodes": 50,
  "model_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\saved_models\\p0_seed43\\P0_gold_20251230_034122\\checkpoints\\best_model.pth",
  "deterministic": false,
  "seed_eval": 43,
  "episode_set": null,
  "success_rate": 0.72,
  "stall_rate": 0.28,
  "mean_progress_final": 0.8286575376262042,
  "max_abs_contour_error": 0.1740128056350915,
  "has_non_finite": false,
  "thresholds": {
    "success_rate_ge": 0.8,
    "stall_rate_le": 0.05,
    "mean_progress_final_ge": 0.95,
    "max_abs_contour_error_le": 0.75
  },
  "half_epsilon": 0.75,
  "timestamp": "2025-12-31 11:23:14",
  "config_path": "C:\\Users\\wumin\\Nutstore\\1\\DDPG的轨迹平滑\\基于强化学习的轨迹平滑\\PPO_project\\configs\\train_square_p0_seed43.yaml"
}
[run] D:\Anaconda\envs\PPO\python.exe C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\main.py --mode train --config C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed42.yaml --experiment_name p0_seed42 --experiment_dir C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed42\P0_gold_20251230_034122 --resume C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed42\P0_gold_20251230_034122\checkpoints\latest_checkpoint.pth
[run] D:\Anaconda\envs\PPO\python.exe C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\tools\acceptance_suite.py --phase p0_eval --config C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed42.yaml --model C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed42\P0_gold_20251230_034122\checkpoints\best_model.pth --episodes 50 --out C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\artifacts\p0_eval_seed42_P0_gold_20251230_034122 --seed 42
[run] D:\Anaconda\envs\PPO\python.exe C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\main.py --mode train --config C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed43.yaml --experiment_name p0_seed43 --experiment_dir C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122 --resume C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\latest_checkpoint.pth
[run] D:\Anaconda\envs\PPO\python.exe C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\tools\acceptance_suite.py --phase p0_eval --config C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\configs\train_square_p0_seed43.yaml --model C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\saved_models\p0_seed43\P0_gold_20251230_034122\checkpoints\best_model.pth --episodes 50 --out C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\artifacts\p0_eval_seed43_P0_gold_20251230_034122 --seed 43
[warn] p0_eval exited non-zero (likely failed thresholds). Using existing summary.json at C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_project\artifacts\p0_eval_seed43_P0_gold_20251230_034122\summary.json.
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[ENV] Effective parameters (from YAML or defaults):
  dt (interpolation_period): 0.001
  MAX_VEL=100.0, MAX_ACC=2000.0, MAX_JERK=20000.0, MAX_ANG_VEL=6.283185307179586, MAX_ANG_ACC=100.0, MAX_ANG_JERK=1000.0
[done] P0_gold archived at: C:\Users\wumin\Nutstore\1\DDPG的轨迹平滑\基于强化学习的轨迹平滑\PPO_FINAL_OPTIMIZATION_PLAN\00_Archive\P0_gold\P0_gold_20251230_034122
