这是一份为你准备的、发给 Codex (或任何 AI 编程助手/开发人员) 的优化指令。这份指令不包含具体代码片段，而是从**网络结构**、**环境交互逻辑**、**奖励函数设计**三个核心维度进行了详细的逻辑拆解。

你可以直接复制以下内容发送：

***

### 优化任务：修复 PPO 轨迹跟踪项目无法直线行驶及收敛问题

**背景描述：**
我将一个基于规则的强约束单文件脚本重构为了模块化的工程项目（`PPO_project`）。旧版本虽然死板但能走直线，新版本引入了更真实的积分控制和前瞻观测，但目前训练效果极差，智能体甚至无法学会走直线，经常原地不动或直接撞墙。

**根本原因分析：**
经过排查，新项目存在三个致命的逻辑缺陷：**动作空间映射错误**（导致死火）、**物理单位未还原**（导致控制失效）、**奖励函数过于稀疏**（导致缺乏方向感）。

请严格按照以下三个步骤的逻辑对代码进行修正：

#### 第一步：修正策略网络（PolicyNet）的输出层逻辑
**问题：** 当前策略网络（Actor）的线速度和角速度分支都使用了 `Tanh` 激活函数（输出范围 -1 到 1）。
**逻辑修正要求：**
1.  **线速度分支（Linear Velocity）：** 这里的物理含义是速度的大小，不能为负。目前的 `Tanh` 会输出负数，导致被环境截断为 0，智能体有一半的概率在“倒车/停车”，导致梯度消失。
    * *修改指令：* 将线速度分支的最终激活函数从 `Tanh` 改为 `Sigmoid`，使其输出范围固定在 `[0, 1]`，代表从静止到最大速度的比例。
2.  **角速度分支（Angular Velocity）：** 保持 `Tanh` 不变，因为转向需要左右（正负）控制。
3.  **移除限制：** 确保线速度分支没有被错误的逻辑强制映射回负区间。

#### 第二步：修正环境步进（Environment Step）中的动作缩放
**问题：** PPO 网络输出的是归一化的数值（0~1 或 -1~1），但环境中的运动学解算器（Kinematics Solver）期望的是物理单位（m/s 或 rad/s）。当前代码直接将网络输出丢给了物理引擎，导致控制量极其微小，无法驱动车辆。
**逻辑修正要求：**
1.  **显式缩放（Explicit Scaling）：** 在 `step` 函数接收到动作后，必须立即进行物理单位还原。
    * 将网络的**角速度输出**乘以 `MAX_ANG_VEL`（最大角速度）。
    * 将网络的**线速度输出**乘以 `MAX_VEL`（最大线速度）。
2.  **传递修正：** 将缩放后的“物理动作”传递给运动学约束函数和状态更新函数，而不是使用原始的网络输出。

#### 第三步：重构奖励函数（Reward Shaping）以解决“盲区”问题
**问题：** 当前的“走廊奖励（Corridor Reward）”逻辑存在漏洞。在安全区内，奖励只与速度挂钩，与位置无关。这导致智能体认为“只要不撞墙，斜着走也可以”，从而无法学会保持在路径中心，最终导致画龙或撞墙。
**逻辑修正要求：**
1.  **引入“居中系数（Centering Factor）”：** 修改走廊奖励计算逻辑。即使在安全区（Safe Zone）内，也要根据横向误差（Contour Error）对速度奖励进行衰减。
    * *逻辑：* 如果误差为 0（在正中心），获得 100% 的速度奖励；如果误差接近安全边界，速度奖励应显著降低。这会产生一个“引力”，引导智能体主动回到中心线。
2.  **新增“航向对齐奖励（Heading Alignment Reward）”：** 在原有奖励基础上，显式增加一项关于方向的奖励。
    * *逻辑：* 计算当前车头朝向与路径切线方向的夹角。夹角越小，奖励越大（建议使用指数形式衰减）。这是教会智能体“走直线”最直接的反馈信号。
3.  **保留积分控制模式：** 不需要退回到旧版的“相对角度控制”，但在有了上述两项奖励（居中+对齐）后，智能体就能在积分控制模式下学会直线行驶，并在弯道处为了获取更高速度奖励而自动学会“切弯”。

#### （可选）第四步：优化观测空间
**建议：** 当前的前瞻点（Lookahead Points）数量过多（20个），导致输入维度过大且信息冗余，增加了训练难度。建议将其减少（例如减至 5-8 个），但保持甚至扩大前瞻的总距离，通过增大采样间隔来实现。这有助于加快收敛速度。

***

**执行目标：**
修改后的代码应当让智能体在训练初期（前几百个 Step）就能通过“对齐奖励”和“居中系数”快速学会沿参考路径直线行驶，随后在训练中探索出利用容差带进行切弯的高级策略。