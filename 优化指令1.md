# Role
你现在是一名资深的强化学习算法工程师，专精于代码重构与调试。

# Context
我现在遇到了一个严重的移植问题：
1. 我的单文件脚本 `PPO最终版_改进.py` 虽然不够智能，但能正常跑通，轨迹基本合理。
2. 我将其重构为模块化项目 `PPO_project` 后，训练效果极差（不收敛、震荡、乱跑）。

# Task: 核心参数与逻辑的“像素级”对齐
请忽略所有高级优化技巧，现在的首要任务是**找不同**。请读取 `PPO最终版_改进.py` 和 `PPO_project` 的相关文件，执行以下强制对齐操作：

## 1. 奖励函数对齐 (Critical)
对比 `PPO最终版_改进.py` 中的 `calculate_reward` 函数与 `src/environment/reward.py`。
- **检查项：** 所有的权重系数（$\alpha$ 轮廓误差, $\beta$ 进给速度, $\gamma$ 捷度/加速度惩罚）必须完全一致。
- **特别注意：** 检查是否有“隐性惩罚”遗漏了？例如老代码在 `done=True` 时是否给了一个巨大的负奖励（如 -100），而新代码里漏掉了？
- **Action：** 输出两者差异，并提供修改后的 `src/environment/reward.py` 代码，使其逻辑与老脚本完全一致。

## 2. PPO 超参数对齐
对比 `PPO最终版_改进.py` 中的 PPO 参数与 `configs/default.yaml`。
- **检查项：**
    - `learning_rate`: 如果老代码是 `1e-4`，新配置千万不要用 `3e-4`。
    - `ent_coef` (熵系数): 过高的熵会导致动作持续抖动。
    - `batch_size` & `n_steps`: 采样长度是否改变？
    - `net_arch`: 网络层数和每层神经元数量是否一致？
- **Action：** 修改 `configs/default.yaml`，使其参数值严格等于老脚本的设定。

## 3. 初始状态对齐
检查环境的 `reset()` 方法。
- **检查项：** 老代码的起始点是固定的还是随机的？新代码是否引入了过大的随机性导致初期难以学习？
- **Action：** 确保新环境的初始化逻辑与老代码保持一致。

请只关注“恢复原状”，不要进行“改进”。输出分析结果和修正后的代码。