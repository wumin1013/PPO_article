# 优化指令 02：双轨存档与断点续训机制

## 背景
训练过程漫长，我们需要两套存档逻辑：一套用于防崩溃（保留优化器状态），一套用于发论文（只保留最优模型权重）。

## 任务
重构 `main.py` 的训练循环和 `src/utils/checkpoint.py`。

## 执行要求
1.  **双轨存档策略 (Dual Checkpointing):**
    * **策略 A (For Resume):** `latest_checkpoint.pth`
        * *频率:* 每 N 步（如 2048 步）强制覆写。
        * *内容:* 完整现场（`model`, `optimizer`, `scheduler`, `step`, `obs_normalizer_stats`）。
    * **策略 B (For Paper):** `best_model.pth`
        * *频率:* 仅当 `eval_reward` 创新高时保存。
        * *内容:* 仅权重 (`model_state_dict`)，轻量化。

2.  **断点续训逻辑 (Resume Logic):**
    * 增加参数 `--resume path/to/latest_checkpoint.pth`。
    * **核心逻辑:** 加载权重 -> 恢复优化器 -> **将 CSV Logger 设为追加模式 (Append Mode)** -> 恢复步数计数器。

## 自验证
1. 运行训练 50 步后手动中断 (Ctrl+C)。
2. 使用 `--resume` 重启训练。
3. **断言:** TensorBoard/CSV 中的 `step` 计数应从 51 开始连续记录，而不是重置为 0。