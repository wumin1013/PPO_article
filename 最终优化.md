------

这是一个为了让 AI 编程助手（如 Codex、Cursor、Copilot）能够一步步完美执行而设计的**终极指令方案**。

它将之前的“环境重构”与“算法升级”合并为一条连贯的逻辑链，确保 **先打地基（解耦），再建高楼（算法优化）**。

请直接将以下 markdown 内容复制并发送给你的 AI 助手。

------

# 优化指令 07：核心重造行动 (Operation Core Rebirth) —— 环境解耦与算法升级

## 1. 背景与目标 (Context)

目前项目存在两个严重阻碍训练效果的问题：

1. **架构依赖：** `src/environment/cnc_env.py` 仍然依赖旧文件 `legacy_env.py`，导致逻辑无法彻底修改。
2. **算法缺陷：** Agent 缺乏输入归一化和有效的探索机制，导致在轨迹 10%-20% 处反复失败（起步即崩溃）。

**目标：** 按顺序执行“环境解耦”和“算法内核升级”，彻底移除旧代码，并引入 **RSI (随机复位)** 和 **Running Mean Std (动态归一化)** 技术。

------

## 2. 第一阶段：环境重构与去遗留化 (Environment Refactoring)

**任务目标：** 彻底移除对 `legacy_env.py` 的依赖，将几何计算模块化，并在新环境中预留随机复位接口。

### 步骤 1.1：抽离几何计算库

操作文件： 创建新文件 src/utils/geometry.py

执行动作：

从 src/environment/legacy_env.py 中提取以下几何计算方法，将其转换为无状态的独立函数（删除 self 参数）：

1. `generate_offset_paths(Pm, epsilon)`: 生成左右边界 Pl, Pr。
2. `is_point_in_polygon(point, polygon)`: 射线法判断点是否在多边形内。
3. `point_to_line_distance(pt, A, B)`: 原 `_helen_formula_distance`，计算点到直线的垂直距离。
4. `project_point_to_segment(pt, p1, p2)`: 计算点在线段上的投影点。
5. `find_intersection(...)`: 计算直线交点辅助函数。

### 步骤 1.2：重写核心环境类

操作文件： src/environment/cnc_env.py

执行动作：

完全重写 Env 类，严禁 import legacy_env。需按以下逻辑组装：

1. **引用更新：**

   - `from src.utils.geometry import ...` (引入步骤 1.1 的工具)
   - `from src.environment.kinematics import apply_kinematic_constraints`
   - `from src.environment.reward import RewardCalculator`

2. **移植核心逻辑：**

   - 将 `legacy_env.py` 中的 `__init__`, `step`, `_precompute_and_cache_geometric_features` 等核心逻辑复制过来。
   - 将所有原本调用 `self.method_name` 的几何计算，替换为调用 `geometry.function_name`。

3. **实现 RSI (随机复位) 接口 (关键)：**

   - 修改 `reset` 方法签名：`def reset(self, random_start: bool = False):`

   - 实现逻辑：

     Python

     ```
     if random_start and len(self.Pm) > 100:
         # 随机选择索引（避开最后 10%）
         start_idx = np.random.randint(0, int(len(self.Pm) * 0.9))
         # 强制设置当前状态
         self.current_segment_idx = start_idx
         self.current_position = np.array(self.Pm[start_idx])
         # 根据当前点重新计算切向角
         self._current_direction_angle = self._get_path_direction(self.current_position)
         # 这里的 trajectory 也要重置为当前点
         self.trajectory = [self.current_position.copy()]
     else:
         # 原有的从起点 (0,0) 开始的逻辑
         pass
     ```

### 步骤 1.3：清理旧文件

**执行动作：**

- **永久删除** `src/environment/legacy_env.py`。
- 检查并删除 `src/environment/__pycache__`。

------

## 3. 第二阶段：算法内核升级 (Algorithm Upgrade)

**任务目标：** 解决数值敏感度问题，实施课程学习。

### 步骤 2.1：引入动态特征归一化

操作文件： src/utils/rl_utils.py

执行动作：

新增两个类用于在线标准化（Online Normalization）：

1. `class RunningMeanStd`: 维护 `mean`, `var`, `count`，使用 Welford 算法进行增量更新。
2. `class StateNormalizer`:
   - `__init__(self, state_dim)`
   - `__call__(self, state, update=True)`: 更新统计量并返回 `(state - mean) / (std + eps)`。

### 步骤 2.2：修正 PPO 网络结构

操作文件： src/algorithms/ppo.py

执行动作：

1. **动作空间限制 (Tanh)：**
   - 在 `PolicyNetContinuous` 的 `forward` 方法中，对输出的均值 `mu` (包括角度和速度分支) 强制应用 `torch.tanh(mu)`。
   - *说明：这将确保神经网络输出严格限制在 [-1, 1] 区间，防止动作发散。*
2. **移除错误的奖励归一化：**
   - 在 `PPOContinuous.update` 方法中，**删除** `rewards = (rewards - rewards.mean()) / ...` 这一行。
   - *说明：只保留 Advantage 的归一化，奖励归一化在 Batch 较小时会破坏信号。*

### 步骤 2.3：升级训练主循环与配置

**操作文件：** `main.py` 和 `configs/default.yaml`

**A. 修改 configs/default.yaml：**

- `batch_size`: 修改为 **256** (原 64，归一化需要大 Batch)。
- `actor_lr`: 修改为 **0.0001** (原 3e-4，归一化后梯度更敏感，需降低)。

**B. 修改 main.py (实施课程学习)：**

1. **初始化：** 在 `env` 创建后，实例化 `normalizer = StateNormalizer(state_dim)`.

2. **归一化注入：**

   - `state = env.reset(...)` 后立即执行 `state = normalizer(state)`.
   - `next_state, r, d, _ = env.step(...)` 后立即执行 `next_state = normalizer(next_state)`.

3. **课程学习逻辑 (Curriculum Loop)：**

   - 在训练循环中加入：

   Python

   ```
   # 前 30% 的 Episode 使用随机起步，快速探索各种弯道情况
   use_random_start = (episode < total_episodes * 0.3)
   state = env.reset(random_start=use_random_start)
   ```

------

## 4. 自验证清单 (Verification Checklist)

请在完成代码修改后，按以下标准自检：

1. **依赖检查：** 全局搜索 `legacy_env`，确保没有任何文件引用它。
2. **启动检查：** 运行 `python main.py`，程序正常启动，无 Import 错误。
3. **行为观察：**
   - 在前 30% 的训练日志中，观察 `step_log.csv` 或控制台输出。
   - Agent 的初始位置 (Position) 应该在轨迹的各个不同路段跳变，而不是每次都从 (0,0) 开始。
4. **收敛迹象：** 虽然初期 Reward 波动大（因为随机出生），但在关闭随机起步（后 70% 阶段）后，Agent 应能稳定跑过之前卡死的 10% 弯道点。
